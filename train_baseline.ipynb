{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_baseline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzwcEjaUI1iBYsw1nLZk75"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1KSB0N1oAhC"
      },
      "source": [
        "import os\n",
        "code_path = os.getcwd()\n",
        "data_path = code_path + 'Original X-ray Image Dataset'\n",
        "covid_path = data_path + '/covid/'\n",
        "none_path = data_path + '/none/'\n",
        "pneumonia_path = data_path + '/pneumonia/'\n",
        "print(len(os.listdir(covid_path))) #125\n",
        "print(len(os.listdir(none_path))) #500\n",
        "print(len(os.listdir(pneumonia_path))) #500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzxU5yQMoUeM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models, layers, losses, optimizers, utils, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "from livelossplot import PlotLossesKeras\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import glob\n",
        "from sklearn.utils import class_weight\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4wP_VCrocnt"
      },
      "source": [
        "covid = glob.glob(covid_path+'*.*')\n",
        "pneumonia = glob.glob(pneumonia_path+'*.*')\n",
        "none = glob.glob(none_path+'*.*')\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "def image_preprocessing(i):\n",
        "  image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "  target_size= (256,256))\n",
        "  image=np.asarray(image)\n",
        "  image = image.astype('float32')/255\n",
        "  data.append(image)\n",
        "\n",
        "for img in covid:\n",
        "  image_preprocessing(img)\n",
        "  labels.append(0)\n",
        "for img in none:\n",
        "  image_preprocessing(img)\n",
        "  labels.append(1)\n",
        "for img in pneumonia:\n",
        "  image_preprocessing(img)\n",
        "  labels.append(2)\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob4X8GTTp5W6"
      },
      "source": [
        "def makemodel(k):\n",
        "      global model\n",
        "      K.clear_session()\n",
        "      model_input = layers.Input(shape = (256,256,3))\n",
        "      x = layers.Conv2D(8, 3, 1, padding='same', use_bias=False)(model_input)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.MaxPooling2D((2,2),strides=2)(x)\n",
        "      x = layers.Conv2D(16, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.MaxPooling2D((2,2),strides=2)(x)\n",
        "      x = layers.Conv2D(32, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.ZeroPadding2D(((2,0),(2,0)))(x)\n",
        "      x = layers.Conv2D(16, 1, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.Conv2D(32, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.MaxPooling2D((2,2),strides=2)(x)\n",
        "      x = layers.Conv2D(64, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.ZeroPadding2D(((2,0),(2,0)))(x)\n",
        "      x = layers.Conv2D(32, 1, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.Conv2D(64, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.MaxPooling2D((2,2),strides=2)(x)\n",
        "      x = layers.Conv2D(128, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.ZeroPadding2D(((2,0),(2,0)))(x)\n",
        "      x = layers.Conv2D(64, 1, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.Conv2D(128, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.MaxPooling2D((2,2),strides=2)(x)\n",
        "      x = layers.Conv2D(256, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.ZeroPadding2D(((2,0),(2,0)))(x)\n",
        "      x = layers.Conv2D(128, 1, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.Conv2D(256, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.ZeroPadding2D(((2,0),(2,0)))(x)\n",
        "      x = layers.Conv2D(128, 1, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.Conv2D(256, 3, 1, padding='same', use_bias=False)(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "      x = layers.Conv2D(k, k, 1, padding=\"same\")(x)\n",
        "      x = layers.ReLU()(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.Flatten()(x)\n",
        "      model_output = layers.Dense(2, activation=\"softmax\")(x)\n",
        "      model = Model(model_input, model_output) \n",
        "      return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaykAxm8of0k"
      },
      "source": [
        "def train_model(k):\n",
        "    BS = 32\n",
        "    \n",
        "    seed = 7\n",
        "    numpy.random.seed(seed)\n",
        "    X = data\n",
        "    y = labels\n",
        "    fold = 1\n",
        "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "    \n",
        "    for train, test in kfold.split(X,y):\n",
        "        makemodel(k)\n",
        "        model.summary()\n",
        "\n",
        "        initial_learning_rate = 3e-3\n",
        "        decay_steps = 500\n",
        "        decay_rate = 0.6\n",
        "        learning_rate_fn = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "          initial_learning_rate, decay_steps, decay_rate)\n",
        "    \n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn),\n",
        "                  loss= 'categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "        y[train] = tf.keras.utils.to_categorical(y[train], 3)\n",
        "        y[test] = tf.keras.utils.to_categorical(y[test], 3)\n",
        "\n",
        "        sampleweights = sklearn.utils.class_weight.compute_sample_weight('balanced', y[train])\n",
        "        history = model.fit(X[train], y[train],\n",
        "                                batch_size = BS,\n",
        "                                epochs = 100,\n",
        "                                validation_data = (X[test], y[test]),\n",
        "                                callbacks=[PlotLossesKeras()],\n",
        "                                sample_weight = sampleweights)\n",
        "        print(history.history.keys())\n",
        "\n",
        "        model.save(data_path+'baseline_3class_model_'+fold+'.h5')\n",
        "        model.save_weights(data_path+'baseline_3class_modelweights_'+fold+'.h5')\n",
        "\n",
        "        test_set = X[test]\n",
        "        predIdxs = model.predict(test_set,\n",
        "          batch_size=BS)\n",
        "        predIdxs = np.argmax(predIdxs, axis=1)\n",
        "        y[test] = np.argmax(y[test], axis=1)\n",
        "\n",
        "        def plot_cm(y_true, y_pred, figsize=(10,10)):\n",
        "            cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
        "            cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "            cm_perc = cm / cm_sum.astype(float) * 100\n",
        "            annot = np.empty_like(cm).astype(str)\n",
        "            nrows, ncols = cm.shape\n",
        "            for i in range(nrows):\n",
        "                for j in range(ncols):\n",
        "                    c = cm[i, j]\n",
        "                    p = cm_perc[i, j]\n",
        "                    if i == j:\n",
        "                        s = cm_sum[i]\n",
        "                        annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "                    elif c == 0:\n",
        "                        annot[i, j] = ''\n",
        "                    else:\n",
        "                        annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "\n",
        "            cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
        "            cm.index.name = 'Actual'\n",
        "            cm.columns.name = 'Predicted'\n",
        "            fig, ax = plt.subplots(figsize=figsize)\n",
        "            sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
        "            plt.savefig(data_path+'baseline_3class_cm_'+fold+'.svg')\n",
        "            plt.show()\n",
        "        plot_cm(y[test], predIdxs)\n",
        "\n",
        "        names = ['COVID','none', 'pneumonia']\n",
        "        print('Classification Report')\n",
        "        print(classification_report(y[test], predIdxs,\n",
        "          target_names=names))\n",
        "        clsf_report = pd.DataFrame(classification_report(y[test], predIdxs,\n",
        "          target_names=names, output_dict=True)).transpose()\n",
        "        clsf_report.to_csv(data_path + 'baseline_3class_report_'+fold+'.csv', index= True)\n",
        "        \n",
        "        fold+=1\n",
        "\n",
        "train_model(3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}